import csv
import model.config as config
import common.utils as utils
from app.annict_get_api import get_works, get_staffs
from app.scraper import scrape_anime_info
from app.notion_register import (
  exists_database_in_page,
  create_anime_info,
  create_database
)


save_dir = "./works_info"
CSV_FILE = "works_info.csv"
file_path = f"./works_info/{CSV_FILE}"
fname = "./data/anime_release_schedule.csv"

# ç¾åœ¨ã®å¹´æœˆæ—¥ã‚’å–å¾—
year, month, _ = utils.sysdate()
season = utils.get_season(month+1)

def load_url_map() -> dict:
    """CSVã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«-URLãƒãƒƒãƒ—ã‚’èª­ã¿è¾¼ã‚€"""
    url_map = {}
    if not config.os.path.exists(file_path):
        return url_map

    with open(file_path, newline="", encoding="utf-8") as f:
        reader = csv.reader(f)
        next(reader)  # ãƒ˜ãƒƒãƒ€ã‚’é£›ã°ã™
        for row in reader:
            _, title, url, _ = row
            url_map[title] = url
        return url_map


def get_url_map(force_refresh: bool = False) -> dict:
    """å¿…è¦ã«å¿œã˜ã¦APIã‹ã‚‰å–å¾— or CSVã‹ã‚‰èª­ã¿è¾¼ã¿"""
    if force_refresh or not config.os.path.exists(file_path):
        print("ğŸ”„ APIã‹ã‚‰URLãƒãƒƒãƒ—ã‚’å–å¾—ä¸­...")
        works = fetch_url_map_from_api()
        save_works(works)

        url_map = {}
        for title, work in works.items():
            if len(work[0]) == 3:
                _, url, _ = work[0]
                url_map[title] = url
        return url_map
    else:
        print("âœ… ãƒ­ãƒ¼ã‚«ãƒ«CSVã‹ã‚‰URLãƒãƒƒãƒ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return load_url_map()


def save_works(works: dict):
    """ã‚¢ãƒ‹ãƒ¡æƒ…å ±ã‚’ã‚’CSVã«ä¿å­˜"""
    if not config.os.path.exists(save_dir):
        config.os.mkdir(save_dir)

    header = ["work_id", "title", "url", "production"]
    with open(file_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        for title, work in works.items():
            if len(work[0]) == 3:
                work_id, url, production = work[0]
                writer.writerow([work_id, title, url, production])


def url_join(url: str, params: str) -> str:
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    return url + f"access_token={config.ANNICT_TOKEN}" + params.strip()


def fetch_url_map_from_api() -> dict:
    """APIã‹ã‚‰å–å¾—"""

    # ã‚¢ã‚¯ã‚»ã‚¹URLã®æº–å‚™--works--
    # ã‚¯ãƒ¼ãƒ«1ã‹æœˆå‰ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãŸã‚+1ã§èª¿æ•´
    params = f"&filter_season={year}-{season}"
    target_url = url_join(config.ANNICT_WORK_URL, params)

    # AnnictAPIã‚’å®Ÿè¡Œã—ã‚¢ãƒ‹ãƒ¡ã®ä½œå“æƒ…å ±ã€é–¢é€£åˆ¶ä½œä¼šç¤¾ã‚’ã‚’å–å¾—
    works = get_works(target_url)

    # ã‚¢ã‚¯ã‚»ã‚¹URLã®æº–å‚™--staffs--
    params = "&filter_work_id={}"
    target_url = url_join(config.ANNICT_STAFFS_URL, params)
    get_staffs(target_url, works)
    
    return works


# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main() -> None:
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    url_map = get_url_map(force_refresh=False)

    # APIã‚’å¼·åˆ¶çš„ã«å†å–å¾—ã—ãŸã„æ™‚ã ã‘
    # url_map = get_url_map(force_refresh=True)

    # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ å¯¾å¿œè¡¨ã®URLã‚ˆã‚Šæœ€é€Ÿé…ä¿¡ã€Œæ—¥æ™‚ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åã€ã‚’å–å¾—
    earliest_list = scrape_anime_info(url_map)

    # é…ä¿¡æ—¥æ™‚ã‚’CSVã«è¨˜éŒ²
    utils.write_csv(fname, earliest_list)

    # DATABASEã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    database_title = f'{year}{config.convert_season[season]}{config.DATABASE_TITLE}'
    is_db = exists_database_in_page(config.PARENT_PAGE_ID, database_title)
    
    # å­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
    if not is_db:
        db_id = create_database(config.PARENT_PAGE_ID, config.DATABASE_TITLE)
    
    
    # ãƒ¬ã‚³ãƒ¼ãƒ‰ä»¶æ•°ã‚’å–å¾—ã—0ä»¶ã§ãªã„å ´åˆã®ã¿å¾Œç¶šã®å‡¦ç†ã‚’å®Ÿè¡Œ
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ç™»éŒ²ã™ã‚‹ãƒ˜ãƒƒãƒ€æƒ…å ±ã‚’ä½œæˆ


    # Notion.ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ç™»éŒ²
        # ã“ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’è€ƒãˆã‚‹
        # é‡è¤‡ã—ãŸã‚¿ã‚¤ãƒˆãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ã€æœ€é€Ÿæ—¥æ™‚ã®èª¿æ•´ 





# ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ(ãƒ†ã‚¹ãƒˆ)
if __name__ == "__main__":
    main()
