import csv
import model.config as config
import common.utils as utils
from app.annict_get_api import get_works, get_staffs
from app.scraper import scrape_anime_info


save_dir = './works_info'
CSV_FILE = 'works_info.csv'
file_path = f'./works_info/{CSV_FILE}'
filename = "./data/anime_release_schedule.csv"

def load_url_map() -> dict:
    """CSVã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«-URLãƒãƒƒãƒ—ã‚’èª­ã¿è¾¼ã‚€"""
    url_map = {}
    if not config.os.path.exists(file_path):
        return url_map
    
    with open(file_path, newline='', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader) # ãƒ˜ãƒƒãƒ€ã‚’é£›ã°ã™
        for row in reader:
            _, title, url, _ = row
            url_map[title] = url
        return url_map


def get_url_map(force_refresh: bool=False) -> dict:
    """å¿…è¦ã«å¿œã˜ã¦APIã‹ã‚‰å–å¾— or CSVã‹ã‚‰èª­ã¿è¾¼ã¿"""
    if force_refresh or not config.os.path.exists(file_path):
        print("ğŸ”„ APIã‹ã‚‰URLãƒãƒƒãƒ—ã‚’å–å¾—ä¸­...")
        works = fetch_url_map_from_api()
        save_works(works)
        
        url_map = {}
        for title, work in works.items():
            if len(work[0]) == 3:
                _,url,_ = work[0]
                url_map[title] = url 
        return url_map
    else:
        print("âœ… ãƒ­ãƒ¼ã‚«ãƒ«CSVã‹ã‚‰URLãƒãƒƒãƒ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return load_url_map()


def save_works(works: dict):
    """ã‚¢ãƒ‹ãƒ¡æƒ…å ±ã‚’ã‚’CSVã«ä¿å­˜"""
    if not config.os.path.exists(save_dir):
        config.os.mkdir(save_dir)
    
    header = ['work_id', 'title', 'url', 'production']
    with open(file_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(header)
        for title, work in works.items():
            if len(work[0]) == 3:                
                work_id, url, production = work[0]
                writer.writerow([work_id, title, url, production])



def fetch_url_map_from_api() -> dict:
    """APIã‹ã‚‰å–å¾—"""
    # ç¾åœ¨ã®å¹´æœˆæ—¥ã‚’å–å¾—
    year, month, _ = utils.sysdate()

    # ã‚¢ã‚¯ã‚»ã‚¹URLã®æº–å‚™--works--
    season = utils.get_season(month)
    params = f'access_token={config.ANNICT_TOKEN}&filter_season={year}-{season}'
    work_url = config.ANNICT_WORK_URL + params

    # AnnictAPIã‚’å®Ÿè¡Œã—ã‚¢ãƒ‹ãƒ¡ã®{ã‚¿ã‚¤ãƒˆãƒ«ï¼šå…¬å¼URL}å¯¾å¿œè¡¨ãŠã‚ˆã³ä½œå“æƒ…å ±ã‚’ã‚’å–å¾—
    works = get_works(work_url)
    get_staffs(works)
    return  works




# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main() -> None:
  """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
  url_map = get_url_map(force_refresh=False)
  
  # APIã‚’å¼·åˆ¶çš„ã«å†å–å¾—ã—ãŸã„æ™‚ã ã‘
  # url_map = get_url_map(force_refresh=True)
  # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ å¯¾å¿œè¡¨ã®URLã‚ˆã‚Šæœ€é€Ÿé…ä¿¡ã€Œæ—¥æ™‚ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åã€ã‚’å–å¾—
  earliest_list = scrape_anime_info(url_map)
#   print(earliest_list)
  utils.csv_read(filename, earliest_list)
  # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥

  # NotionAPIã‚’å®Ÿè¡Œã—ã‚¢ã‚¯ã‚»ã‚¹

  # Notionã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã‚€


# ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ(ãƒ†ã‚¹ãƒˆ)
if __name__ == '__main__':
  main()