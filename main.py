import csv
import model.config as config
import common.utils as utils
from app.annict_get_api import get_works, get_staffs
from app.scraper import scrape_anime_info
from app.notion_register import (
  exists_database_in_page,
  create_database,
  insert
)


# ç¾åœ¨ã®å¹´æœˆæ—¥ã‚’å–å¾—
year, month = utils.get_sysdate()[0:2]
# season = utils.get_season(month+1)
season = utils.get_season(month+2) # æ¤œè¨¼ç”¨

WORKS_CSV_FILE = f"{year}_{season}.csv"
SCHEDULE_CSV_FILE = f"{year}_{season}_scrap.csv"
works_file_path = f"./data/works/{WORKS_CSV_FILE}"
schedule_file_path = f"./data/anime_schedule/{SCHEDULE_CSV_FILE}"



def get_url_map(force_refresh: bool = False) -> dict:
    """
    å¿…è¦ã«å¿œã˜ã¦APIã‹ã‚‰å–å¾— or CSVã‹ã‚‰èª­ã¿è¾¼ã¿
    
    Args:
        force_refresh: CSVèª­ã¿è¾¼ã¿ãƒ•ãƒ©ã‚°ï¼ˆTrue:APIå®Ÿè¡Œ False: ãƒ­ãƒ¼ã‚«ãƒ«CSVï¼‰

    Returns:
        {title: [url, production]}ï¼ˆå¯¾å¿œè¡¨ï¼‰
    """
    if force_refresh or not utils.exists_file_path(works_file_path):
        print("ğŸ”„ APIã‹ã‚‰URLãƒãƒƒãƒ—ã‚’å–å¾—ä¸­...")
        works = fetch_works_api()
        save_works(works)
    else:
        print("âœ… ãƒ­ãƒ¼ã‚«ãƒ«CSVã‹ã‚‰URLãƒãƒƒãƒ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    return utils.read_csv(works_file_path)
        


def save_works(works: dict):
    """ã‚¢ãƒ‹ãƒ¡æƒ…å ±ã‚’ã‚’CSVã«ä¿å­˜"""
    
    header = ["work_id", "title", "url", "production"]
    with open(works_file_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        for title, work in works.items():
            if len(work[0]) == 3:
                work_id, url, production = work[0]
                writer.writerow([work_id, title, url, production])


def url_join(url: str, params: str) -> str:
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    return url + f"access_token={config.ANNICT_TOKEN}" + params.strip()


def fetch_works_api() -> dict:
    """APIã‹ã‚‰å–å¾—"""

    # ã‚¢ã‚¯ã‚»ã‚¹URLã®æº–å‚™--works--
    params = f"&filter_season={year}-{season}"
    target_url = url_join(config.ANNICT_WORK_URL, params)

    # AnnictAPIã‚’å®Ÿè¡Œã‚¢ãƒ‹ãƒ¡ã®ä½œå“æƒ…å ±å–å¾—
    works = get_works(target_url)

    # ã‚¢ã‚¯ã‚»ã‚¹URLã®æº–å‚™--staffs--
    params = "&filter_work_id={}"
    target_url = url_join(config.ANNICT_STAFFS_URL, params)
    # åˆ¶ä½œä¼šç¤¾ã‚’ã‚’å–å¾—
    get_staffs(target_url, works)
    
    return works


# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main() -> None:
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    earliest_list = {}
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°CSVã®æœ‰ç„¡ï¼ˆç¾æ™‚ç‚¹ã§ã¯æ‰‹å‹•å‰Šé™¤ï¼‰
    if not utils.exists_file_path(schedule_file_path):
        url_map = get_url_map(force_refresh=False)
        # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã€Œé…ä¿¡æ—¥æ™‚ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€ã‚’å–å¾—
        earliest_list = scrape_anime_info(url_map)
        # æœ€é€Ÿé…ä¿¡æƒ…å ±ã‚’CSVã«è¨˜éŒ²
        utils.write_csv(schedule_file_path, earliest_list)
        earliest_list = utils.read_csv(schedule_file_path, mode=2)
    
    # DBã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    database_name = f'{year}{config.convert_season[season]}{config.DATABASE_NAME}'
    db_id = exists_database_in_page(config.PARENT_PAGE_ID, database_name)
    
    # å­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼‹è¡Œè¿½åŠ 
    if db_id is None:
        # æ–°è¦DBä½œæˆæ™‚ã«å¤ã„DBã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–åŒ–ã—ã¦é‹ç”¨ï¼ˆè«–ç†å‰Šé™¤ï¼‰
        db_id = create_database(config.PARENT_PAGE_ID, config.DATABASE_NAME, [year, month])
        insert(earliest_list, db_id)





# ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ(ãƒ†ã‚¹ãƒˆ)
if __name__ == "__main__":
    main()
