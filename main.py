import csv
import model.config as config
import common.utils as utils
from app.annict_get_api import get_works, get_staffs
from app.scraper import scrape_anime_info


save_dir = "./works_info"
CSV_FILE = "works_info.csv"
file_path = f"./works_info/{CSV_FILE}"
filename = "./data/anime_release_schedule.csv"


def load_url_map() -> dict:
    """CSVã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«-URLãƒãƒƒãƒ—ã‚’èª­ã¿è¾¼ã‚€"""
    url_map = {}
    if not config.os.path.exists(file_path):
        return url_map

    with open(file_path, newline="", encoding="utf-8") as f:
        reader = csv.reader(f)
        next(reader)  # ãƒ˜ãƒƒãƒ€ã‚’é£›ã°ã™
        for row in reader:
            _, title, url, _ = row
            url_map[title] = url
        return url_map


def get_url_map(force_refresh: bool = False) -> dict:
    """å¿…è¦ã«å¿œã˜ã¦APIã‹ã‚‰å–å¾— or CSVã‹ã‚‰èª­ã¿è¾¼ã¿"""
    if force_refresh or not config.os.path.exists(file_path):
        print("ğŸ”„ APIã‹ã‚‰URLãƒãƒƒãƒ—ã‚’å–å¾—ä¸­...")
        works = fetch_url_map_from_api()
        save_works(works)

        url_map = {}
        for title, work in works.items():
            if len(work[0]) == 3:
                _, url, _ = work[0]
                url_map[title] = url
        return url_map
    else:
        print("âœ… ãƒ­ãƒ¼ã‚«ãƒ«CSVã‹ã‚‰URLãƒãƒƒãƒ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return load_url_map()


def save_works(works: dict):
    """ã‚¢ãƒ‹ãƒ¡æƒ…å ±ã‚’ã‚’CSVã«ä¿å­˜"""
    if not config.os.path.exists(save_dir):
        config.os.mkdir(save_dir)

    header = ["work_id", "title", "url", "production"]
    with open(file_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        for title, work in works.items():
            if len(work[0]) == 3:
                work_id, url, production = work[0]
                writer.writerow([work_id, title, url, production])


def url_join(url: str, params: str) -> str:
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    return url + f"access_token={config.ANNICT_TOKEN}" + params.strip()


def fetch_url_map_from_api() -> dict:
    """APIã‹ã‚‰å–å¾—"""
    # ç¾åœ¨ã®å¹´æœˆæ—¥ã‚’å–å¾—
    year, month, _ = utils.sysdate()

    # ã‚¢ã‚¯ã‚»ã‚¹URLã®æº–å‚™--works--
    season = utils.get_season(month)
    params = f"&filter_season={year}-{season}"
    target_url = url_join(config.ANNICT_WORK_URL, params)

    # AnnictAPIã‚’å®Ÿè¡Œã—ã‚¢ãƒ‹ãƒ¡ã®ä½œå“æƒ…å ±ã€é–¢é€£åˆ¶ä½œä¼šç¤¾ã‚’ã‚’å–å¾—
    works = get_works(target_url)

    # ã‚¢ã‚¯ã‚»ã‚¹URLã®æº–å‚™--staffs--
    params = "&filter_work_id={}"
    target_url = url_join(config.ANNICT_STAFFS_URL, params)
    get_staffs(target_url, works)

    # get_staffs(works)
    return works


# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main() -> None:
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    url_map = get_url_map(force_refresh=False)

    # APIã‚’å¼·åˆ¶çš„ã«å†å–å¾—ã—ãŸã„æ™‚ã ã‘
    # url_map = get_url_map(force_refresh=True)

    # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ å¯¾å¿œè¡¨ã®URLã‚ˆã‚Šæœ€é€Ÿé…ä¿¡ã€Œæ—¥æ™‚ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åã€ã‚’å–å¾—
    earliest_list = scrape_anime_info(url_map)

    # é…ä¿¡æ—¥æ™‚ã‚’CSVã«è¨˜éŒ²
    utils.csv_write(filename, earliest_list)

    # NotionAPIã‚’å®Ÿè¡Œã—ã‚¢ã‚¯ã‚»ã‚¹
    # Notionã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã‚€


# ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ(ãƒ†ã‚¹ãƒˆ)
if __name__ == "__main__":
    main()
